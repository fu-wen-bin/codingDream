# LLM 是什么

本质上就是一个函数，输入一段文本，输出另一段文本。

# RAG 是什么 (Retrieval Augmented Generation) -- 生成式检索增强

1. 搜索引擎

   自己的向量数据库，豆包就可以在向量数据库中做数据检索

2. 超长上下文大模型全文嵌入

   moonshot 128k 上下文长度

3. 对结构化、语义化的文件系统结合目录搜索与文件读取

   逐步搜索，AI-coding 工具常用的方式

## 以上三种方法的局限性

- 搜索引擎搜索的内容是公开的，并且搭建搜索引擎的成本较高
- 能接受超长上下文大模型全文嵌入的大模型运行成本较高。其次如果需要多轮对话，每一轮都要带上之前的对话历史，会增加成本。所以这种手段只适用于单论对话
- 对结构化搜索，往往需要多轮执行，也会消耗大量token，

# RAG 技术 (推荐)

经典的RAG技术，结合了信息检索和语音生成的

# 向量数据库和向量检索

embedding 模型，将数据压缩成语义向量，通过向量的相似度（余弦距离，欧氏距离，点积等）来进行匹配，从而真正实现语义化搜索。

所以向量检索才能做到真正意义上的语义匹配

# 如何构建向量检索系统

1. 选择合适的embedding模型（构建向量的模型：nomic-embed-text）
   ```bash
   ollama pull nomic-embed-text
   ```
2. 构建向量数据库 (vectra: 一个基于Nodejs的微型向量数据库)
   企业一般使用node-faiss来构建向量数据库，faiss是facebook开源的向量数据库，支持cpu和gpu加速
3. pnpm i ollama -- 安装 ollama SDK 
4. pnpm i vectra -- 安装 vectra 向量数据库
5. 做RAG检索将高相关的内容作为参考资料，输入给大模型，大模型根据参考资料进行回答